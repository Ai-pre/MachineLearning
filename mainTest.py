# ============================================
# meta_preprocessed.csv ÏÉùÏÑ± (CBÏö© Feature Í∞ïÌôî & Score Ìè¨Ìï® Ïä§ÏºÄÏùºÎßÅ)
# + TF-IDF / Encoder / Scaler Ï†ÄÏû•
# ============================================

import pickle
import pandas as pd
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text import TfidfVectorizer

# -----------------------------
# 1Ô∏è‚É£ Îç∞Ïù¥ÌÑ∞ Î°úÎìú Î∞è Í≤∞Ï∏° Ï≤òÎ¶¨
# -----------------------------
meta_df = pd.read_csv("anime.csv")
print(f"‚úÖ Loaded anime.csv, shape: {meta_df.shape}")

# Ï£ºÏöî Í≤∞Ï∏°Í∞í Ï±ÑÏö∞Í∏∞
meta_df = meta_df.fillna({
    'Genres': '',
    'Producers': '',
    'Studios': '',
    'Licensors': '',
    'Type': 'Unknown',
    'Source': 'Unknown',
    'Rating': 'Unknown',
    'Premiered': 'Unknown',
    'Duration': 'Unknown'
})

# MAL_ID Ï†ïÍ∑úÌôî
meta_df['MAL_ID'] = meta_df['MAL_ID'].astype(int)

# -----------------------------
# 2Ô∏è‚É£ Î≤îÏ£ºÌòï Ïù∏ÏΩîÎî© (Ï†ÄÏû• Í∞ÄÎä•ÌïòÎèÑÎ°ù ÏàòÏ†ï)
# -----------------------------
label_cols = ['Type', 'Source', 'Rating', 'Premiered', 'Duration']
label_encoders = {}

for col in label_cols:
    le = LabelEncoder()
    meta_df[f'{col}_encoded'] = le.fit_transform(meta_df[col].astype(str))
    label_encoders[col] = le

# -----------------------------
# 3Ô∏è‚É£ TF-IDF ÌîºÏ≤ò ÏÉùÏÑ± (fit Í∞ùÏ≤¥ÎèÑ Ìï®Íªò Ï†ÄÏû•)
# -----------------------------
def make_tfidf_features(df, column, prefix):
    tfidf = TfidfVectorizer(
        token_pattern=r'[^, ]+',
        stop_words='english',
        max_features=100
    )
    mat = tfidf.fit_transform(df[column])
    tfidf_df = pd.DataFrame(
        mat.toarray(),
        columns=[f"{prefix}_{t}" for t in tfidf.get_feature_names_out()]
    )
    return tfidf_df, tfidf

tfidf_genres, vec_genres = make_tfidf_features(meta_df, 'Genres', 'Genre')
tfidf_producers, vec_producers = make_tfidf_features(meta_df, 'Producers', 'Prod')
tfidf_studios, vec_studios = make_tfidf_features(meta_df, 'Studios', 'Studio')

# -----------------------------
# 4Ô∏è‚É£ ÏàòÏπòÌòï Ïª¨Îüº Ï≤òÎ¶¨ (Score Ìè¨Ìï®)
# -----------------------------
numeric_cols = ['Score', 'Episodes', 'Ranked', 'Popularity', 'Members', 'Favorites']
for col in numeric_cols:
    meta_df[col] = pd.to_numeric(meta_df[col], errors='coerce')
meta_df[numeric_cols] = meta_df[numeric_cols].fillna(meta_df[numeric_cols].mean())

scaler = MinMaxScaler()
meta_df[numeric_cols] = scaler.fit_transform(meta_df[numeric_cols])

# -----------------------------
# 5Ô∏è‚É£ CBÏö© Feature Í≤∞Ìï©
# -----------------------------
meta_processed = pd.concat(
    [
        meta_df[['MAL_ID'] + numeric_cols + [f'{col}_encoded' for col in label_cols]],
        tfidf_genres,
        tfidf_producers,
        tfidf_studios
    ],
    axis=1
)

# -----------------------------
# 6Ô∏è‚É£ rating_complete.csvÏôÄ Îß§Ïπ≠ÎêòÎäî IDÎßå ÌïÑÌÑ∞ÎßÅ
# -----------------------------
rating_df = pd.read_csv("rating_complete.csv")
rating_df['anime_id'] = rating_df['anime_id'].astype(int)

valid_ids = set(rating_df['anime_id']).intersection(set(meta_processed['MAL_ID']))
meta_processed = meta_processed[meta_processed['MAL_ID'].isin(valid_ids)].reset_index(drop=True)

print(f"‚úÖ Matched items with rating data: {len(valid_ids)} / {len(meta_df)}")
print(f"‚úÖ Final processed shape: {meta_processed.shape}")

# -----------------------------
# 7Ô∏è‚É£ Ï†ÄÏû• (Îç∞Ïù¥ÌÑ∞ + Í∞ùÏ≤¥)
# -----------------------------
meta_processed.to_csv("meta_preprocessed.csv", index=False)
print("üíæ Saved: meta_preprocessed.csv")

artifacts = {
    "label_encoders": label_encoders,
    "tfidf_genre": vec_genres,
    "tfidf_prod": vec_producers,
    "tfidf_studio": vec_studios,
    "scaler": scaler,
    "numeric_cols": numeric_cols,
    "label_cols": label_cols
}

with open("encoders_scalers.pkl", "wb") as f:
    pickle.dump(artifacts, f)
print("üíæ Saved: encoders_scalers.pkl")

#---------------------------------------------------------------------------------------------------


# ============================================
# anime_test.csv ‚Üí meta_preprocessed_test.csv 
# ============================================

import pandas as pd
import numpy as np
import pickle

# -----------------------------
# 1Ô∏è‚É£ ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ Î°úÎìú
# -----------------------------
meta_test = pd.read_csv("anime_test.csv")
print(f"‚úÖ Loaded anime_test.csv, shape: {meta_test.shape}")

# Ïª¨ÎüºÎ™Ö ÌÜµÏùº
if "anime_id" in meta_test.columns:
    meta_test.rename(columns={"anime_id": "MAL_ID"}, inplace=True)

# Í≤∞Ï∏° Ï≤òÎ¶¨
meta_test = meta_test.fillna({
    'genre': '',
    'type': 'Unknown'
})

# ID Ï†ïÏàòÌòï Î≥ÄÌôò
meta_test['MAL_ID'] = meta_test['MAL_ID'].astype(int)

# -----------------------------
# 2Ô∏è‚É£ trainÏóêÏÑú ÌïôÏäµÎêú TF-IDF / Scaler Î∂àÎü¨Ïò§Í∏∞
# -----------------------------
with open("encoders_scalers.pkl", "rb") as f:
    artifacts = pickle.load(f)

tfidf_genre = artifacts["tfidf_genre"]
scaler = artifacts["scaler"]

# -----------------------------
# 3Ô∏è‚É£ TF-IDF Î≥ÄÌôò (train vocab Í∏∞Î∞ò)
# -----------------------------
from sklearn.feature_extraction.text import TfidfVectorizer

# trainÍ≥º Ïª¨ÎüºÎ™ÖÏù¥ Îã§Î•¥ÎØÄÎ°ú genre Ïª¨ÎüºÎßå Îß§Ìïë
mat = tfidf_genre.transform(meta_test['genre'])
tfidf_df = pd.DataFrame(
    mat.toarray(),
    columns=[f"Genre_{t}" for t in tfidf_genre.get_feature_names_out()]
)

# -----------------------------
# 4Ô∏è‚É£ Numeric Scaling (train Ïä§ÏºÄÏùºÎü¨ ÏùºÎ∂Ä ÏÇ¨Ïö©)
# -----------------------------
# train numeric_colsÏóêÏÑú ÍµêÏßëÌï©Îßå ÏÇ¨Ïö©
numeric_cols = ['rating', 'episodes', 'members']
for col in numeric_cols:
    meta_test[col] = pd.to_numeric(meta_test[col], errors='coerce')
meta_test[numeric_cols] = meta_test[numeric_cols].fillna(meta_test[numeric_cols].mean())

# ÏÉàÎ°úÏö¥ scaler ÌïòÎÇò Îçî ÎßåÎì§Ïñ¥ Ï†ÅÏö© (ÎèÖÎ¶ΩÏ†Å ÌÖåÏä§Ìä∏ÏÖãÏù¥ÎØÄÎ°ú)
from sklearn.preprocessing import MinMaxScaler
scaler_test = MinMaxScaler()
meta_test[numeric_cols] = scaler_test.fit_transform(meta_test[numeric_cols])

# -----------------------------
# 5Ô∏è‚É£ Feature Í≤∞Ìï© Î∞è Ï†ÄÏû•
# -----------------------------
meta_test_processed = pd.concat(
    [meta_test[['MAL_ID'] + numeric_cols], tfidf_df],
    axis=1
)

meta_test_processed.to_csv("meta_preprocessed_test.csv", index=False)
print("üíæ Saved: meta_preprocessed_test.csv")




#-----------------------------------------------------------------------------------------------------
# ============================================
# Hybrid Recommender Evaluation (Test set: rating_test.csv + meta_preprocessed_test.csv)
# ============================================

import numpy as np
import pandas as pd
from surprise import Dataset, Reader, SVD
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# -----------------------------
# 1Ô∏è‚É£ Train + Test Data Load
# -----------------------------
train_df = pd.read_csv("rating_complete.csv")
test_df = pd.read_csv("rating_test.csv")
meta_train = pd.read_csv("meta_preprocessed.csv")
meta_test = pd.read_csv("meta_preprocessed_test.csv")

train_df = train_df[train_df['rating'] > 0]
test_df = test_df[test_df['rating'] > 0]

print(f"‚úÖ Train shape: {train_df.shape}, Test shape: {test_df.shape}")

# -----------------------------
# 2Ô∏è‚É£ CF Model (SVD, trainÏúºÎ°úÎßå ÌïôÏäµ)
# -----------------------------
reader = Reader(rating_scale=(train_df['rating'].min(), train_df['rating'].max()))
data = Dataset.load_from_df(train_df[['user_id', 'anime_id', 'rating']], reader)
trainset = data.build_full_trainset()

svd = SVD(n_factors=100, n_epochs=15, random_state=42, verbose=True)
svd.fit(trainset)

# -----------------------------
# 3Ô∏è‚É£ Content Matrix (train+test merge)
# -----------------------------
meta_all = pd.concat([meta_train, meta_test], ignore_index=True).drop_duplicates("MAL_ID")
# üîπ NaN Í∞í Ï†ÑÎ∂Ä 0ÏúºÎ°ú ÎåÄÏ≤¥ (CB Ïú†ÏÇ¨ÎèÑ Í≥ÑÏÇ∞ Ïãú ÏïàÏ†Ñ)
meta_all = meta_all.fillna(0.0)

feature_cols = [c for c in meta_all.columns if c != 'MAL_ID']
scaler = StandardScaler()
X = scaler.fit_transform(meta_all[feature_cols].values)
malid_to_idx = {aid: i for i, aid in enumerate(meta_all['MAL_ID'].values)}

# -----------------------------
# 4Ô∏è‚É£ Scoring Functions
# -----------------------------
def cf_score(user_id, anime_id):
    try:
        return svd.predict(user_id, anime_id).est
    except:
        return 0.0

def cb_score(user_id, anime_id, like_th=7.0):
    if anime_id not in malid_to_idx:
        return 0.0
    user_hist = train_df[train_df['user_id'] == user_id]
    liked = user_hist[user_hist['rating'] >= like_th]['anime_id'].values
    liked_idxs = [malid_to_idx[a] for a in liked if a in malid_to_idx][:30]
    if len(liked_idxs) == 0:
        return 0.0

    v = X[malid_to_idx[anime_id]].reshape(1, -1)
    L = X[liked_idxs]
    sims = cosine_similarity(L, v).flatten()
    return float(np.mean(sims))

def hybrid_score(user_id, anime_id, alpha=0.7, like_th=7.0):
    cf = cf_score(user_id, anime_id)
    cb = cb_score(user_id, anime_id, like_th)
    return alpha * cf + (1 - alpha) * cb

# -----------------------------
# 5Ô∏è‚É£ Precision / Recall (Test set)
# -----------------------------
def precision_recall_fast(user_id, scoring_fn, k=10, like_th=7.0, sample_items=3000):
    user_test = test_df[test_df['user_id'] == user_id]
    actual_liked = set(user_test[user_test['rating'] >= like_th]['anime_id'])
    if len(actual_liked) == 0:
        return None, None

    rated_items = set(train_df.loc[train_df['user_id'] == user_id, 'anime_id'])
    all_items = [a for a in meta_all['MAL_ID'].values if a not in rated_items]

    if len(all_items) > sample_items:
        np.random.seed(42)
        all_items = np.random.choice(all_items, sample_items, replace=False)

    scores = [(aid, scoring_fn(user_id, aid)) for aid in all_items]
    topk = [aid for aid, s in sorted(scores, key=lambda x: x[1], reverse=True)[:k]]
    hits = len(set(topk) & actual_liked)
    return hits / k, hits / len(actual_liked)

# -----------------------------
# 6Ô∏è‚É£ Evaluation Wrapper
# -----------------------------
def evaluate_fast(model_name, scoring_fn, user_sample=15, k_values=[5, 10]):
    valid_users = train_df.groupby('user_id').size()
    active_users = valid_users[valid_users >= 10].index
    users = test_df[test_df['user_id'].isin(active_users)]['user_id'].drop_duplicates().sample(user_sample, random_state=42)

    results = []
    for k in k_values:
        precisions, recalls = [], []
        for u in tqdm(users, desc=f"{model_name} (Top-{k})"):
            p, r = precision_recall_fast(u, scoring_fn, k=k)
            if p is not None:
                precisions.append(p)
                recalls.append(r)
        results.append((model_name, k, np.mean(precisions), np.mean(recalls)))
        print(f"\nüìä {model_name} (Top-{k}) ‚Üí P@{k}: {np.mean(precisions):.4f}, R@{k}: {np.mean(recalls):.4f}")

    return results

# -----------------------------
# 7Ô∏è‚É£ Evaluate CF / CB / Hybrid (Œ±-grid)
# -----------------------------
alphas = [0.3, 0.5, 0.7, 0.9]
results = []

# CF-only
results += evaluate_fast("CF-only", cf_score, user_sample=15, k_values=[5, 10])

# CB-only
results += evaluate_fast("CB-only", cb_score, user_sample=15, k_values=[5, 10])

# Hybrid Œ±-grid
for a in alphas:
    fn = lambda u, aid, alpha=a: hybrid_score(u, aid, alpha=alpha)
    results += evaluate_fast(f"Hybrid (Œ±={a})", fn, user_sample=15, k_values=[5, 10])

# -----------------------------
# 8Ô∏è‚É£ Í≤∞Í≥º Ï†ïÎ¶¨
# -----------------------------
res_df = pd.DataFrame(results, columns=["Model", "Top-K", "Precision", "Recall"])
print("\nüìä Model Comparison (Test Set, Top-5 & Top-10):")
print(res_df)
